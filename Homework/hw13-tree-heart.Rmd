---
title: "Predicting heart disease with classification trees"
author: "Sean Vucinich"
date: "April 23, 2017"
output:
  html_document: default
  word_document: default
---

<!-- change echo=FALSE to echo=TRUE to show code -->
```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
```

```{r collapse=TRUE, warning=FALSE}
library(rpart)
library(rpart.plot)
library(maptree)
# the following utility files can be found attached to the assignment
source("https://raw.githubusercontent.com/grbruns/cst383/master/lin-regr-util.R")
source("https://raw.githubusercontent.com/grbruns/cst383/master/class-util.R")
```

### Reading and preprocessing the data

The first thing we do is we read in the data set and do some pre-processing upon the data.

```{r}
heart = read.table("https://raw.githubusercontent.com/grbruns/cst383/master/heart.dat", quote = "/")
names(heart) <- c("AGE", "SEX", "CHESTPAIN", "RESTBP", "CHOL",
                  "SUGAR", "ECG", "MAXHR", "ANGINA", "DEP", "EXERCISE", "FLUOR",
                  "THAL", "OUTPUT")
names(heart) = tolower(names(heart))

# convert output to factor
heart$output = factor(heart$output)
```

### Data exploration

To begin the data exploration we want to take the summary of the data to see what we are working with. After understanding what features are included in the data set, we can determine what features we want to use in the first tree model.

```{r}
  #str(heart)
  #summary(heart)
```

We should create some plots to observe some of the data for the sake of data exploration. In an attmept to observe something interesting in the data set, we'll create a histogram of the ages(age) and a scatterplot of the cholesterol amounts versus the output condition of heart disease being present (chol and output).

```{r}
hist(heart$age, col = "#FF8822", main = "Distribution of Ages in the Data Set", xlab = "Age")
```

Looking at the histogram of the ages, we can observe that the majority of the ages in the population of the data set are between 40 and 70, with only a handful of the data points existing outside of that range.

```{r}
plot(heart$chol, heart$output, col = c("#FF8822"), main = "Cholesterol vs. Heart Disease", xlab = "Cholesterol", ylab = "Output")
```

Looking at this plot, we can see the distribution of heart disease being present in people with different Cholesterol amounts. For the y-axis, a y value of 1 is no heart disease and 2 is heart disease.

### Building a classification tree

Before we start building a classifcation tree, we need to split the data set up so we have data to train and test upon. The split_data function was defined in one of the source files (lin-regr-util.R), and we use a seed of 132 in order to allow for the results to be re-producible.

```{r}
# training and test sets
set.seed(132)
split = split_data(heart)
tr_dat = split[[1]]
te_dat = split[[2]]
```

We will now build the first classification tree, using 3 features to create a tree that we can work to improve upon in the second model. These features will be chestpain, dep, and maxhr.

```{r}
fit = rpart(output ~ chestpain + dep + maxhr,data=tr_dat, method="class")
```

We can look at the summary of the model to see which features of the model are significant.

```{r}
#summary(fit)
```

From the summary, we can see our features used in the model are all decently significant. Now we will plot the tree to see what the fitted tree actually looks like.

```{r}
par(mar = c(1,1,3,1))
prp(fit, extra=106, varlen=-10,main="Classification Tree for Model 1",box.col=c("#FF8822", "#2288FF")[fit$frame$yval])
```

From looking at the fitted tree and the summary, we can see that chestpain is a very significant feature in the prediction of the presence of heart disease. Dep and maxhr are also significant features, however are not as significant as chestpain is. 

### Classifying test data

Now we run the model using the training and test data in order to generate predicitions. The training data was used in the creation of the model, and the test data is used now in the function predict in order to generate those predicitions.

```{r}
predicted = predict(fit, te_dat, type="class")
actual = te_dat$output
```


### Assessing the model

To begin assessing the effectiveness of the model, we will first calculating the accuracy of the model and also calculating the confusion matrix of the predictions.

```{r}
success_rate = mean(predicted == actual)
round(success_rate,3)

conf_matrix = table(actual, predicted)
conf_matrix
```

The initial accuracy of the model is 75.4%, which is an ok accuracy for a first model but it can be improved upon. We can also calculate the false positive and false negative rates of the model in order to determine what bias, if any, the model has.

```{r}
false_positive_rate = mean(predicted == 1 & actual == 2)
round(false_positive_rate,3)
false_negative_rate = mean(predicted == 2 & actual == 1)
round(false_negative_rate,3)
```

The false positive and negative rates of the model are both above 10% which is more then we would like. Now we will create a learning curve of the model to determine if the model has a high bias or a high varience.

```{r}

tr_errs = c()
te_errs = c()
te_actual = te_dat$output
tr_sizes = seq(100, nrow(tr_dat), length.out=10)
for (tr_size in tr_sizes) 
{
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$output
  
  fit1 = rpart(output ~ chestpain + dep + maxhr,data=tr_dat1, method="class")

  # error on training set
  tr_predicted = predict(fit1, tr_dat1, type="class")
  err = mean(tr_actual != tr_predicted)
  tr_errs = c(tr_errs, err)
  
  # error on test set
  te_predicted = predict(fit1, te_dat, type="class")
  err = mean(te_actual != te_predicted)
  te_errs = c(te_errs, err)
}

plot(tr_sizes, tr_errs, col = "#FF8822", type = "b", ylim = c(0,0.5), main = "Learning Curve", xlab = "Training Set Size", ylab = "Training Errors")
lines(tr_sizes, te_errs, col = "#2288FF", type = "b")
legend("topright", legend = c("Test Data", "Training Data"), col = c("#2288FF", "#FF8822"), pch = 19)
```

Looking at the learning curve plot, it would appear this model is more prone to high bias.

### Model 2

Using what was learned from the previous model, we can now build the second classification tree, to improve on the results of the first model. This time, we will use 3 features to create a tree that has higher accuracy then the first model. These features will be chestpain, thal, fluor, and maxhr.

```{r}
fit2 = rpart(output ~ chestpain + thal + fluor, data = tr_dat, method = "class")
```

We can look at the summary of the model to see which features of the model are significant.

```{r}
#summary(fit2)
```

From the summary, we can see our features used in the model are all decently significant, with the range feature significance of each feature being closer than the first model. Now we will plot the tree to see what the fitted tree actually looks like.

```{r}
par(mar = c(1,1,3,1))
prp(fit2, extra=106, varlen=-10,main="Classification Tree for Model 2",box.col=c("#FF8822", "#2288FF")[fit$frame$yval])
```

From looking at the fitted tree and the summary, we can see that chestpain is the most significant feature in the prediction of the presence of heart disease. This result corroborated by the fact that in the previous model, chestpain was also the most significant feature. Flour and thal are also significant features, with their significance being similar. 

### Classifying test data

Now we run the model using the training and test data in order to generate predicitions. The training data was used in the creation of the model, and the test data is used now in the function predict in order to generate those predicitions.

```{r}
predicted = predict(fit2, te_dat, type="class")
actual = te_dat$output
```


### Assessing the model

To begin assessing the effectiveness of the model, we will first calculating the accuracy of the model and also calculating the confusion matrix of the predictions.

```{r}
success_rate = mean(predicted == actual)
round(success_rate,3)

conf_matrix = table(actual, predicted)
conf_matrix
```

The initial accuracy of the model is 82.6%, which is significantly more accurate than the first model. We can also calculate the false positive and false negative rates of the model in order to determine what bias, if any, the model has.

```{r}
false_positive_rate = mean(predicted == 1 & actual == 2)
round(false_positive_rate,3)
false_negative_rate = mean(predicted == 2 & actual == 1)
round(false_negative_rate,3)
```

The false positive and negative rates of the model are 7.02% and 10.1% respectively, which are less than the rate of the first model, but still higher then preferred. Now we will create a learning curve of the model to determine if the model has a high bias or a high varience.

```{r}

tr_errs = c()
te_errs = c()
te_actual = te_dat$output
tr_sizes = seq(100, nrow(tr_dat), length.out=10)
for (tr_size in tr_sizes) 
{
  tr_dat1 = tr_dat[1:tr_size,]
  tr_actual = tr_dat1$output
  
  fit2 = rpart(output ~ chestpain + thal + fluor, data = tr_dat1, method = "class")

  # error on training set
  tr_predicted = predict(fit1, tr_dat1, type="class")
  err = mean(tr_actual != tr_predicted)
  tr_errs = c(tr_errs, err)
  
  # error on test set
  te_predicted = predict(fit1, te_dat, type="class")
  err = mean(te_actual != te_predicted)
  te_errs = c(te_errs, err)
}

plot(tr_sizes, tr_errs, col = "#FF8822", type = "b", ylim = c(0,0.5), main = "Learning Curve", xlab = "Training Set Size", ylab = "Training Errors")
lines(tr_sizes, te_errs, col = "#2288FF", type = "b")
legend("topright", legend = c("Test Data", "Training Data"), col = c("#2288FF", "#FF8822"), pch = 19)
```

Looking at the learning curve plot, it would appear this model is more prone to high bias. Compared to the first model however, the second model is more prone to bias while at the same time having an overall lower error rate.
