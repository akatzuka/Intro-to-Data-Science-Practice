
---
title: "Predicting used car prices with linear regression"
author: "Glenn Bruns"
date: "April 5, 2018"
output: html_document
---

<!-- Homework instructions:  PLEASE READ CAREFULLY
     Problems are indicated in HTML comments.
     Below each problem is a code "chunk" containing the R comment 
     # YOUR CODE HERE
     Replace the comment with one or more lines of code.
     Do not change the file except in this way!  And do not remove
     comments that start with #@
-->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(prompt=TRUE, comment="", echo=TRUE)
```

```{r}
set.seed(123)
```

We will use the Kuipers 2008 used car data to try to predict the price
of a used car from features like mileage, engine size, and trim.

<!-- problem 1. 
"source" the file "lin-regr-util.R" that is attached to the homework assignment
-->
```{r}
#@ 1
source("lin-regr-util.R")
```

<!-- read the data -->

<!-- problem 2. 
Write R code to read the csv file into a data frame
and assign it to variable 'cars'.  Use R function 'read.csv'.
https://raw.githubusercontent.com/grbruns/cst383/master/kuiper-2008-cars.csv
-->
```{r}
#@ 2
cars <- read.csv("https://raw.githubusercontent.com/grbruns/cst383/master/kuiper-2008-cars.csv")
```

<!-- preprocess data -->

```{r}
# notes:
#  - mileage means total number of miles
#  - R treats factors as indicator variables, automatically

# turn Doors into an indicator variable
cars$FourDoor = as.numeric(cars$Doors == 4)
cars$Doors = NULL

# turn Cylinder into an indicator variable
cars$cyl6 = as.numeric(cars$Cylinder == 6)
cars$cyl8 = as.numeric(cars$Cylinder == 8)
cars$Cylinder = NULL

# turn Type into indicator variable (use all but one to avoid correlation)
cars$Convertible = as.numeric(cars$Type == "Convertible")
cars$Coupe = as.numeric(cars$Type == "Coupe")
cars$Hatchback = as.numeric(cars$Type == "Hatchback")
cars$Sedan = as.numeric(cars$Type == "Sedan")
cars$Type = NULL

# don't need make as it is implied by Model, but put
# Make at the front of Model
cars$Model = paste(cars$Make, cars$Model)
cars$Make = NULL
```
### Data exploration

In a real project to predict used car prices, we would spend
substantial effort on pre-processing and exploring our data.
Here we show only scatter plots of a few features.

<!-- problem 3.
produce a grid of scatterplots for the cars data, but using only 
Price, Mileage, and Liter
-->
```{r}
#@ 3
pairs(~ cars$Price + cars$Mileage + cars$Liter)
```

### Simple linear regression to predict Price from Mileage

Let's try predicting price from a car's mileage, which means
the number of miles the car has been driven.

<!-- problem 4.
using lm, perform simple linear regression to create a model
that estimates Price using feature Mileage.  Assign your
model to variable 'fit'.
-->
```{r}
#@ 4
fit = lm(Price ~ Mileage, data = cars)
```

The fit shows decreasing price with increasing mileage, as we expect.
However, the scatterplot shows we can't expect too much from using
mileage along.


<!-- problem 5.
produce a scatter plot of Price (y axis) by Mileage (x axis), and
superimpose the linear model you created as a dotted line on the
scatterplot
-->
```{r}
#@ 5
plot(cars$Price, cars$Mileage)
abline(fit)
```

Summary information from the model fit shows large error and
a small R-squared statistic.

```{r}
# the summary of the fit shows high error
summary(fit)
```

```{r}
# plots of the fit show that the distribution of residuals is
# not too far from a normal distribution
plot(fit)
```

### Predicting price from Mileage, Cruise, and Leather

We expand the model by incorporating whether a car has
a leather interior and cruise control.

<!-- problem 6.
Using lm, create a linear model that estimates Price using 
features Mileage, Cruise, and Leather.  
Assign your model to variable 'fit2'.
-->
```{r}
#@ 6
fit2 <- lm(Price ~ Mileage + Cruise + Leather, data = cars)
```

```{r}
# R-squared statistic is not good, but better
summary(fit2)
```

Plots of predicted vs. actual prices
help in showing how the model does overall, and also whether
the error is constant or not.  In these plots we see that
error tends to be bigger when the price is higher.

<!-- problem 7.
produce a scatter plot in which you plot the actual
used car prices (y axis) against the predicted used
car prices.  Plot a dotted line showing where the
points would be if the predicted values equalled the
actual values.  Give your plot an appropriate title.
-->
```{r}
#@ 7
predicted <- predict(fit2, cars)
actual <- cars$Price
rng <- range(c(predicted,actual))
plot(actual ~ predicted, pch = 16, col = "#FF8888", ylim = rng, main = "Predicted vs. Actual Values")
lines(c(rng[1], rng[2]),c(rng[1], rng[2]),lty=2, col="blue", lwd=1.5)
```

<!-- problem 8.
produce a similar scatter plot, but this time use
function 'plot_predict_actual' that is defined in 
file 'lin-regr-util.R' (attached to the homework
assignment).  Do not modify this file.  Use 2000 for 
the error band parameter, and use an appropriate title.
-->
```{r}
#@ 8
plot_predict_actual(predicted,actual, 2000, "Used Car Prices: Actual by Predicted")
```

The Residuals vs Fitted plot is another way of seeing how errors increase with price.  This tells us our linear model is not very good yet.  The Q-Q plot shows that the 
distrbution of errors is not too far from a normal distribution.

```{r}
plot(fit2)
```

```{r}
# We want our model to fits our data well, but especially want
# a model that will make good predictions with *future* data.  In other
# words, we want our model to generalize well.  To get an idea
# of how our model will generalize, we split our data into 
# training and test data.
#
# split data into test and training sets using function 'split_data'
# from the lin-regr-util.R file.  The second parameter of the function
# shows how many files the data frame of the first parameter should
# be split into, and their relative sizes.  In this usage the training/test
# data sets use a 75/25 split.
dsets = split_data(cars, c(3,1))
tr_dat = dsets[[1]]
te_dat = dsets[[2]]
```


<!-- problem 9.
Using lm, create another linear model that estimates Price using 
features Mileage, Cruise, and Leather.  However, this time
fit your model to the training data set tr_dat.  
Assign your model to variable 'fit3'.
-->
```{r}
#@ 9
fit3 <- lm(Price ~ Mileage + Cruise + Leather, data = tr_dat)
```

### Predictions from training data and test data

Looking at prediction errors using the training data is not very useful.
A model that has low error on training data could still have high error
on test data.  In this case we see the error on the test data is similar
to the error on the training data.  This suggests we're not overfitting
when we build our model.

<!-- problem 10.
Using function 'plot_predict_actual', plot the actual
and predicted values from your model.  Get actual
and predicted values based on the *training data*.
-->
```{r}
#@ 10
predicted <- predict(fit3, tr_dat)
actual <- tr_dat$Price
plot_predict_actual(predicted, actual, 2000, "Predictions From Training Data")
```

<!-- problem 11.
Do the same thing as the last problem, but this time,
get actual and predicted values based on the test data.
-->
```{r}
#@ 11
predicted <- predict(fit3, te_dat)
actual <- te_dat$Price
plot_predict_actual(predicted, actual, 2000, "Predictions From Test Data")
```

<!-- problem 12.
calculate the RMSE based on the test data, and assign
it to variable 'rmse'.
-->
```{r}
#@ 12
rmse = sqrt(mean((actual - predicted)^2))
```

The RMSE value computed from our test set confirms that our model is
not yet very useful.


```{r}
print(paste0("RMSE for Price prediction from Mileage, Cruise, Leather, on test data: ",
             round(rmse)))
```

### Adding input feature cyl8 to the linear model

We add to our model a feature that indicates whether a car has
an eight-cylinder engine.

<!-- problem 13.
Create another linear model by adding feature 'cyl8' to
the features used in your last model.  Use the training
data set to create your model.
Assign your model to variable 'fit4'.
-->
```{r}
#@ 13
fit4 <- lm(Price ~ Mileage + Cruise + Leather + cyl8, data = tr_dat)
```

The R-squared statistic is improving.  It looks like leather isn't
very useful for predicting price (at least, in the presence of the
other features we're using).

```{r}
summary(fit4)
```

The distribution of the residuals is improving, but it is still
not very much like a normal distribution.

<!-- problem 14.
Plot the distribution of the residuals using a density plot
-->
```{r}
#@ 14
plot(density(fit4$residuals), main = "Distribution of Residuals")
```

<!-- problem 15.
Using function 'plot_predict_actual', plot the actual
and predicted values from your model.  Get actual
and predicted values based on the *testing* data.
-->
```{r}
#@ 15
predicted <- predict(fit4, te_dat)
actual <- te_dat$Price
plot_predict_actual(predicted, actual, 2000, "Actual vs. Predicted Prices")

```

### Using a validation data set

A problem is that we can't use the test data to pick the "best"
set of features, because then we can't use the test data to give
a fair idea of the how our model will do on future data.

We also can't use the training data for this purpose, as we already know.

An alternative is to split our data set into *three* pieces, one for
training, another (called the "validation" data set) for comparing
alternative sets of features, and the last for doing the final test
of our model.
 
A problem with splitting the data three ways is that the size of each
of the three pieces is not very big.  This means our validation results
are not very reliable.

<!-- problem 16.
Here's an example showing a 50/25/25 split of the data set into training,
validation, and test sets.  We train a model on the training data, then
test the feature set using the validation data.  Every time we do this,
we compute the RMSE.

In the code below (at the YOUR CODE HERE comment), compute the RMSE based on the validation data set, and assign it to variable 'rmse'
-->
```{r}
rmses = c()
for (i in 1:100) {
  dsets = split_data(cars, c(2, 1, 1))
  train_dat = dsets[[1]]
  valid_dat = dsets[[2]]
  
  fit=lm(Price ~ Mileage + Cruise + Leather + Sound, data=train_dat)
  
  #@ 16
  predicted <- predict(fit, valid_dat)
  actual <- valid_dat$Price
  rmse <- sqrt(mean((actual - predicted)^2))
  
  rmses = c(rmses, rmse)
}

# Note the range of RMSE values that you get
hist(rmses, col="red4", main="Histogram of RMSE values from multiple validations")
```

### Finding best features using cross validation

A way to get more reliable validation results is to "cross validate",
which basically allows the training data to be re-used.  One cross validation
method is called 10-fold cross validation.  In this method you split the
training data into 10 pieces.  To see how well a set of features works,
you train the model on 9 of the 10 pieces, and validate on the remaining piece.
You then repeat this 9 more times, each time using a different subset of 9
of the 10 pieces.  In each of the 10 validation runs, you compute a result,
like the RMSE.

```{r}
# The function "cross_validate_lm" in the file lin-regr-util.R does
# cross validation for feature selection in linear regression.
# if you run the following line of R code a bunch of times, you'll see 
# that the results are quite stable

rmse_cv = cross_validate_lm(tr_dat, "Price", c("Mileage", "Cruise", "Leather", "Sound"))
print(paste0("RMSE from cross validation: ",round(rmse_cv)))
```

Notes on cross validation:

* Cross validation is used for all kinds of machine learning problems,
   not just feature selection in linear regression.
* Please read the cross_validate_lm code and understand how it works.
* There are special methods that allow for very cheap cross validation
   with linear regression.  We're using a general-purpose cross validation
   method here so that you can see the idea.

By computing RMSE for each single feature (using cross validation), we
can find the single most effective feature in predicting price.

<!-- problem 17.
What is the best single feature to use?  Let's write a loop
to test all the features and see which gives the best RMSE.
Replace the YOUR CODE HERE comment with code to set variables
'min_rmse' and 'min_feature' (the name of the feature associated
with min_rmse) as required.
-->
```{r}

features = setdiff(names(cars), "Price")

min_rmse = 100000
for (feature in features) {
  rmse = cross_validate_lm(tr_dat, "Price", feature)
  
  #@ 17
  # YOUR CODE HERE  (assign min_rmse and min_feature as required)
  if(min_rmse > rmse)
  {
    min_rmse <- rmse
    min_feature <- feature
  }
}

paste0("best feature to predict Price: '", min_feature, "'; RMSE = ", round(min_rmse))
```
<!-- problem 18.
Using function 'plot_predict_actual', plot the actual
and predicted values from the model using the best feature
Get actual and predicted values based on the *testing* data.
-->
```{r}
ff = reformulate(min_feature, "Price")
fit = lm(ff, tr_dat)

#@ 18
predicted <- predict(fit, te_dat)
actual <- te_dat$Price

plot_predict_actual(predicted, actual, 2000, "Actual vs. Predicted: Feature Model")

```

```{r}
# R-squared statistic is much better than before
summary(fit)
```

Having found the single best feature, we can look for the best
feature out of the remaining features.  This is a kind of greedy
algorithm for feature selection: we find the best feature, then
the best of the remaining features, etc.

This approach is not guaranteed to find the two best features, however.
Think of trying to find the best basketball team by finding the best
basketball player, then the next-best basketball player, etc.  The
top five people may not work together to be the best team.

<!-- problem 19.
What is best second feature to use?  Let's write a loop
to test all the features and see which gives the best RMSE.
Have your loop assign values to variables min2_rmse and min2_feature.
-->
```{r}
# Trim feature not used because of rank coefficient problems
features = setdiff(names(cars), c("Price", "Trim", min_feature))

#@ 19
min2_rmse = 100000
for (feature in features) {
  rmse = cross_validate_lm(tr_dat, "Price", feature)
  if(min2_rmse > rmse)
  {
    min2_rmse <- rmse
    min2_feature <- feature
  }
}

```

<!-- problem 20.
Repeat problem 18, but this time using the two features found
so far.
-->
```{r}
ff = reformulate(c(min_feature, min2_feature), "Price")
fit = lm(ff, tr_dat)

#@ 20
predicted <- predict(fit, te_dat)
actual <- te_dat$Price

plot_predict_actual(predicted, actual, 2000, "Actual vs. Predicted: Feature Model, cyl8")
  
```

<!-- Extra credit
# Use the idea of the last few problems to find the four "best" features
# for predicting price.  Use a loop, and first find the single best feature,
# then find the next best feature given the first best, etc., until you've
# got 4 features.
#
# Assign your four best features to vector 'best_features', with the first
# value in the vector being the best features, the second value being the
# second best, etc.
-->
```{r}
#@ 21

# Trim feature not used because of rank coefficient problems
features = setdiff(names(cars), c("Price", "Trim"))
best_features = c()
for (i in 1:4)
{
  min_rmse = 100000
  for (feature in features) {
    rmse = cross_validate_lm(tr_dat, "Price", feature)
    if(min_rmse > rmse)
    {
      min_rmse <- rmse
      min_feature <- feature
    }
  }
  best_features = c(best_features, min_feature)
  features = setdiff(names(cars), c("Price", "Trim", best_features)) 
}

ff = reformulate(best_features, "Price")
fit = lm(ff, tr_dat)
predicted <- predict(fit, te_dat)
actual <- te_dat$Price

plot_predict_actual(predicted, actual, 2000, "Actual vs. Predicted: 4 Best Features")
  
```




